#ðŸ“ˆ Stock Price Time Series Analysis & Forecasting (MLOps Project)

This project implements an end-to-end production-ready time series forecasting pipeline for stock prices.
It combines classical time series methods, machine learning, and deep learning, wrapped with modern MLOps tools including Airflow, MLflow, Evidently, and Streamlit for deployment and monitoring.

# ==========================
# Project: AAPL Next-Day Close Price Prediction
# Type: Daily Regression
# Full MLSD Pipeline
# ==========================

# 1. Goal Definition
# ------------------
goal = "Predict next-day close price for AAPL using daily OHLCV data"
task = Regression

# 3. Data Collection
# ------------------
data_source = "Yahoo Finance"
ticker = "AAPL"
frequency = "Daily"
lookback_period = "Last 5 years (~1250 rows)"

____________________________________________
âœ… Project Quick Checklist (MLOps)
--------------------------------------------
âœ… Problem: Next-day AAPL price forecasting (time-series regression)
âœ… Metrics: RMSE, MAE, MAPE (offline evaluation; no SLAs)
âœ… Data: Yahoo Finance (yfinance) ingestion with reproducible preprocessing and time-based splits
âœ… Models: SARIMA, Random Forest, XGBoost, LSTM (Keras), LSTM (PyTorch â€“ best)
âœ… Training & Orchestration: Dockerized Apache Airflow pipeline
âœ… Experiment Tracking: MLflow (params, metrics, artifacts)
âœ… Serving: Streamlit app deployed on Render
âœ… Monitoring: Evidently for data drift & performance (manual runs)
âœ… Tradeoffs: Live data re-fetch; retraining triggered manually

___________________________________
ðŸ§± Project Structure (High-Level)
-----------------------------------

stock_lstm_TSA/
â”‚
â”œâ”€â”€ airflow/                  # Airflow (Dockerized)
â”‚   â””â”€â”€ dags/stock_lstm_pipeline.py
â”‚
â”œâ”€â”€ src/                      # Core ML logic
â”‚   â”œâ”€â”€ data/                 # Data fetch & preprocessing
â”‚   â”œâ”€â”€ models/               # PyTorch LSTM model
â”‚   â”œâ”€â”€ training/             # Training + evaluation
â”‚   â”œâ”€â”€ inference/            # Prediction logic
â”‚   â””â”€â”€ monitoring/           # Evidently monitoring
â”‚
â”œâ”€â”€ data/processed/           # Train/Test CSVs (Evidently)
â”œâ”€â”€ models/                   # Trained models & scalers
â”œâ”€â”€ mlruns/                   # MLflow artifacts
â”œâ”€â”€ my_monitoring_data/       # Evidently workspace
â”œâ”€â”€ streamlit/                # Streamlit app (Render)
â””â”€â”€ requirements.txt

__________________________________
ðŸ§  Models Trained & Evaluated
----------------------------------

The following models were implemented, trained, and evaluated:

SARIMA â€“ classical statistical time series model
Random Forest Regressor â€“ tree-based machine learning model
XGBoost Regressor â€“ gradient boosting model
LSTM (TensorFlow / Keras) â€“ deep learning sequence model
LSTM (PyTorch) â€“ deep learning sequence model

âœ… Best-performing model:
LSTM implemented in PyTorch, selected based on validation and test performance.

______________________________
ðŸš€ How to Run the Project
------------------------------

python -m venv stock_env
source stock_env/bin/activate   # macOS/Linux

pip install -r requirements.txt

______________________________________
2ï¸âƒ£ Run Training (local or via Airflow)
--------------------------------------

python src/training/train.py

cd airflow/docker
docker-compose up

http://localhost:8080

_________________________________
ðŸ“Š MLflow â€“ Experiment Tracking
---------------------------------

Start MLflow UI:
mlflow ui

Open in browser:
http://127.0.0.1:5000

Tracked items:

Parameters (epochs, learning rate, batch size)
Metrics (RMSE, MAE, MAPE)
Artifacts (model weights, scaler)

_______________________________________
ðŸ” Evidently â€“ Data & Model Monitoring
---------------------------------------
Run monitoring script:
python src/monitoring/run_monitoring.py

Start Evidently UI:
evidently ui --workspace my_monitoring_data

Open in browser:
http://127.0.0.1:8000

Monitored aspects:

Data drift
Feature statistics
Regression performance (when predictions available)

______________________________
ðŸŒ Streamlit App (Inference)
------------------------------

Run locally:
streamlit run streamlit/app.py

Run on render: 

Github:  https://github.com/AKholman/stock-lstm-pytorch-streamlit-render
Render: https://streamlit-render-lstm-pytorch.onrender.com
Currently, it is suspended to keep free tier sources. It can be actvated at any time. 

Deployment:

The Streamlit app is deployed on Render, loading:
Render: https://streamlit-render-lstm-pytorch.onrender.com
Saved scaler for inverse transformation

____________________________
ðŸ§© Key Technologies
----------------------------

PyTorch â€“ deep learning
Airflow â€“ workflow orchestration
MLflow â€“ experiment tracking
Evidently â€“ monitoring & drift detection
Streamlit â€“ UI & deployment
Docker â€“ Airflow isolation

______________________________
âœ… Project Highlights
------------------------------

Clean separation between training, inference, and monitoring
Production-style MLOps workflow
Reproducible experiments
Model and data monitoring ready for real-world usage



**APPENDICES** 

A) LSTM MODEL BUILDING:

Input shape: (60 timesteps, 6 features)
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€------â”€â”€â”€â”€-â”
â”‚ LSTM(64, return_sequences=True).   â”‚
â”‚ â†’ outputs 64 features per timestep â”‚
â”‚ Output shape: (60, 64)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€-------â”€â”˜
      â–¼
 Dropout(0.2)
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€---------â”€â”€â”€â”€â”€â”
â”‚ LSTM(32, return_sequences=False)     â”‚
â”‚ â†’ outputs only final timestep vector â”‚
â”‚ Output shape: (32,)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€---------â”€â”€â”€â”˜
      â–¼
 Dropout(0.2)
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€---------------------------â”€â”€â”€â”€â”
â”‚ Dense(16, activation='relu')                           â”‚
â”‚ â†’ fully connected layer, learns nonlinear combinations â”‚
â”‚ Output shape: (16,)                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€---------------------------â”€â”€â”€â”€â”€â”€â”€â”€â”˜
      â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€----â”€â”€â”€â”€â”
â”‚ Dense(1)                        â”‚
â”‚ â†’ final prediction (regression) â”‚
â”‚ Output shape: (1,)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€----â”€â”˜


Some details of Pytorch LSTM model traning and testing. 

1.  Using Minâ€“Max scaling we scaled (normalized) the features (X) and the target (y). The scalers are fit only on the training data, then applied to validation and test â€” which prevents information leakage.

2. Sequence creation - 'def create_sequences(X, y, time_steps=60)' : 
Goal of this step - Transform each continuous 1D timeline of features into overlapping time windows (sequences).
Each sequence of time_steps = 60 days becomes one sample for the LSTM, and the label is the target value right after that window. 
Output: we have NumPy arrays (X_train_seq, y_train_seq, etc.).

But PyTorch models can only work with PyTorch tensors with GPU acceleration and automatic differentiation (autograd).

3. So, Step 4 converts all the NumPy arrays (matrix) into PyTorch tensors and prepares them for efficient mini-batch training.
X(rows, features) -> X(rows, timestep, features), i.e. 2D data (matrix) â†’ 3D sequences (tensor)	LSTM needs (batch, time, features). 
DataLoader (a PyTorch utility) breaks the dataset into mini-batches of 32 samples.


4. MODEL DEFINITION:

model = Sequential([
    LSTM(64, return_sequences=True, input_shape=(X_train_seq.shape[1], X_train_seq.shape[2])),
    Dropout(0.2),
    LSTM(32, return_sequences=False),
    Dropout(0.2),
    Dense(16, activation='relu'),
    Dense(1)
])
criterion = nn.MSELoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

ðŸ”¹ Step 4. Data as it flows:

Step	Layer	      Input shape	     Output shape
1	    Input	       (60, 6)	   â†’     (60, 6).      usually this is not considered as a layer
2	    LSTM(64)	   (60, 6)	   â†’     (60, 64)
3	    Dropout(0.2)   (60, 64)    â†’     (60, 64)
4	    LSTM(32)	   (60, 64)	   â†’     (32,)
5	    Dropout(0.2)	(32,)	   â†’     (32,)
6	    Dense(16, ReLU)	(32,)	   â†’     (16,)
7	    Dense(1)	    (16,)	   â†’     (1,)

SUMMARY:
Total layers: 7
Input layer: (60, 6) â†’ 6 features Ã— 60 timesteps
First LSTM layer: 64 neurons (each learning a temporal pattern)
Output layer: 1 neuron (final continuous prediction)


5. One full batch cycle of training: 
    ðŸ”¹ Step 1 â€” Forward pass:
        Input batch â†’ LSTM1 â†’ LSTM2 â†’ fc1 â†’ ReLU â†’ fc2 â†’ Output â†’ Compute prediction.

    ðŸ”¹ Step 2 â€” Compute loss:
        Loss = criterion(output, true_y). (e.g., Mean Squared Error for regression)

    ðŸ”¹ Step 3 â€” Backpropagation:
        Call loss.backward(): 
            â†’ PyTorch automatically computes gradients âˆ‚loss/âˆ‚weight for all layers.

    ðŸ”¹ Step 4 â€” Optimizer update:
        optimizer.step()
            â†’ All layer parameters are adjusted based on gradients.

    ðŸ”¹ Step 5 â€” Next batch:
        LSTM starts fresh with new sequence inputs.
            Gradients from the previous batch are cleared (optimizer.zero_grad()).
                The updated weights now slightly better fit the data â†’ model improves.

